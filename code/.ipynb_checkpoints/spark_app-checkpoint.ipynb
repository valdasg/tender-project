{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'gs://dl-eu-pub-tender/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 10:01:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/04/15 10:02:00 WARN DependencyUtils: Local jar /home/valdas/lib/gcs-connector-hadoop3-2.2.5.jar does not exist, skipping.\n",
      "22/04/15 10:02:00 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/04/15 10:02:00 INFO ResourceUtils: ==============================================================\n",
      "22/04/15 10:02:00 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/04/15 10:02:00 INFO ResourceUtils: ==============================================================\n",
      "22/04/15 10:02:00 INFO SparkContext: Submitted application: test\n",
      "22/04/15 10:02:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/04/15 10:02:00 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/04/15 10:02:00 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/04/15 10:02:00 INFO SecurityManager: Changing view acls to: valdas\n",
      "22/04/15 10:02:00 INFO SecurityManager: Changing modify acls to: valdas\n",
      "22/04/15 10:02:00 INFO SecurityManager: Changing view acls groups to: \n",
      "22/04/15 10:02:00 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/04/15 10:02:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(valdas); groups with view permissions: Set(); users  with modify permissions: Set(valdas); groups with modify permissions: Set()\n",
      "22/04/15 10:02:01 INFO Utils: Successfully started service 'sparkDriver' on port 45595.\n",
      "22/04/15 10:02:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/04/15 10:02:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/04/15 10:02:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/04/15 10:02:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/04/15 10:02:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/15 10:02:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eb3acb57-9214-4649-bb3a-4ac3a245f942\n",
      "22/04/15 10:02:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "22/04/15 10:02:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/15 10:02:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/04/15 10:02:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://gpc-instance.europe-west6-a.c.eu-pub-tender.internal:4040\n",
      "22/04/15 10:02:01 ERROR SparkContext: Failed to add /home/valdas/lib/gcs-connector-hadoop3-2.2.5.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/valdas/lib/gcs-connector-hadoop3-2.2.5.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/15 10:02:02 INFO Executor: Starting executor ID driver on host gpc-instance.europe-west6-a.c.eu-pub-tender.internal\n",
      "22/04/15 10:02:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37531.\n",
      "22/04/15 10:02:02 INFO NettyBlockTransferService: Server created on gpc-instance.europe-west6-a.c.eu-pub-tender.internal:37531\n",
      "22/04/15 10:02:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/04/15 10:02:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, gpc-instance.europe-west6-a.c.eu-pub-tender.internal, 37531, None)\n",
      "22/04/15 10:02:02 INFO BlockManagerMasterEndpoint: Registering block manager gpc-instance.europe-west6-a.c.eu-pub-tender.internal:37531 with 366.3 MiB RAM, BlockManagerId(driver, gpc-instance.europe-west6-a.c.eu-pub-tender.internal, 37531, None)\n",
      "22/04/15 10:02:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, gpc-instance.europe-west6-a.c.eu-pub-tender.internal, 37531, None)\n",
      "22/04/15 10:02:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, gpc-instance.europe-west6-a.c.eu-pub-tender.internal, 37531, None)\n",
      "22/04/15 10:02:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/04/15 10:02:02 INFO SharedState: Warehouse path is 'file:/home/valdas/code/spark-warehouse'.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInRead' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInRead' instead.\n",
      "22/04/15 10:02:02 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
      "22/04/15 10:02:03 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\n",
      "22/04/15 10:02:03 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
      "22/04/15 10:02:03 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInRead' instead.\n",
      "22/04/15 10:02:03 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n"
     ]
    }
   ],
   "source": [
    "credentials_location = '/home/valdas/.google/credentials/google_credentials.json'\n",
    "\n",
    "# set spark configuration\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set('spark.jars', '/home/valdas/lib/gcs-connector-hadoop3-2.2.5.jar') \\\n",
    "    .set('spark.hadoop.google.cloud.auth.service.account.enable', 'true') \\\n",
    "    .set('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_location) \\\n",
    "    .set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\") \\\n",
    "    .set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\") \\\n",
    "    .set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\") \\\n",
    "    .set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\") \n",
    "\n",
    "# set spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set('fs.AbstractFileSystem.gs.impl',  'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS')\n",
    "hadoop_conf.set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "hadoop_conf.set('fs.gs.auth.service.account.json.keyfile', credentials_location)\n",
    "hadoop_conf.set('fs.gs.auth.service.account.enable', 'true')\n",
    "\n",
    "# create spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf = sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField (\"tender_row_nr\", types.IntegerType(), True),          \n",
    "    types.StructField('tender_id', types.StringType(), True),\n",
    "    types.StructField('tender_country', types.StringType(), True),\n",
    "    types.StructField('tender_title', types.StringType(), True),\n",
    "    types.StructField('tender_size', types.StringType(), True),\n",
    "    types.StructField('tender_supplyType', types.StringType(), True),\n",
    "    types.StructField('tender_procedureType', types.StringType(), True),\n",
    "    types.StructField('tender_nationalProcedureType', types.StringType(), True),\n",
    "    types.StructField('tender_mainCpv', types.StringType(), True),\n",
    "    types.StructField('tender_cpvs', types.StringType(), True),\n",
    "    types.StructField('tender_addressOfImplementation_nuts', types.StringType(), True),\n",
    "    types.StructField('tender_year', types.TimestampType(), True),\n",
    "    types.StructField('tender_eligibleBidLanguages', types.StringType(), True),\n",
    "    types.StructField('tender_npwp_reasons', types.StringType(), True),\n",
    "    types.StructField('tender_awardDeadline', types.TimestampType(), True),\n",
    "    types.StructField('tender_contractSignatureDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_awardDecisionDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_bidDeadline', types.TimestampType(), True),\n",
    "    types.StructField('tender_cancellationDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_estimatedStartDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_estimatedCompletionDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_estimatedDurationInYears', types.IntegerType(), True),\n",
    "    types.StructField('tender_estimatedDurationInMonths', types.IntegerType(), True),\n",
    "    types.StructField('tender_estimatedDurationInDays', types.IntegerType(), True),\n",
    "    types.StructField('tender_isEUFunded', types.StringType(), True),\n",
    "    types.StructField('tender_isDps', types.StringType(), True),\n",
    "    types.StructField('tender_isElectronicAuction', types.StringType(), True),\n",
    "    types.StructField('tender_isAwarded', types.StringType(), True),\n",
    "    types.StructField('tender_isCentralProcurement', types.StringType(), True),\n",
    "    types.StructField('tender_isJointProcurement', types.StringType(), True),\n",
    "    types.StructField('tender_isOnBehalfOf', types.StringType(), True),\n",
    "    types.StructField('tender_isFrameworkAgreement', types.StringType(), True),\n",
    "    types.StructField('tender_isCoveredByGpa', types.StringType(), True),\n",
    "    types.StructField('tender_hasLots', types.StringType(), True),\n",
    "    types.StructField('tender_estimatedPrice', types.FloatType(), True),\n",
    "    types.StructField('tender_estimatedPrice_currency', types.StringType(), True),\n",
    "    types.StructField('tender_estimatedPrice_minNetAmount', types.FloatType(), True),\n",
    "    types.StructField('tender_estimatedPrice_maxNetAmount', types.FloatType(), True),\n",
    "    types.StructField('tender_estimatedPrice_EUR', types.FloatType(), True),\n",
    "    types.StructField('tender_finalPrice', types.FloatType(), True),\n",
    "    types.StructField('tender_finalPrice_currency', types.StringType(), True),\n",
    "    types.StructField('tender_finalPrice_minNetAmount', types.FloatType(), True),\n",
    "    types.StructField('tender_finalPrice_maxNetAmount', types.FloatType(), True),\n",
    "    types.StructField('tender_finalPrice_EUR', types.FloatType(), True),\n",
    "    types.StructField('tender_description_length', types.StringType(), True),\n",
    "    types.StructField('tender_personalRequirements_length', types.StringType(), True),\n",
    "    types.StructField('tender_economicRequirements_length', types.StringType(), True),\n",
    "    types.StructField('tender_technicalRequirements_length', types.StringType(), True),\n",
    "    types.StructField('tender_documents_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_awardCriteria_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_corrections_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_onBehalfOf_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_lots_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_publications_count', types.IntegerType(), True),\n",
    "    types.StructField('tender_publications_firstCallForTenderDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_publications_lastCallForTenderDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_publications_firstdContractAwardDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_publications_lastContractAwardDate', types.TimestampType(), True),\n",
    "    types.StructField('tender_publications_lastContractAwardUrl', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_SINGLE_BID', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_CALL_FOR_TENDER_PUBLICATION', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_ADVERTISEMENT_PERIOD', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_PROCEDURE_TYPE', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_DECISION_PERIOD', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_TAX_HAVEN', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_NEW_COMPANY', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_CENTRALIZED_PROCUREMENT', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_ELECTRONIC_AUCTION', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_COVERED_BY_GPA', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_FRAMEWORK_AGREEMENT', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_ENGLISH_AS_FOREIGN_LANGUAGE', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_ADMINISTRATIVE_NOTICE_AND_AWARD_DISCREPANCIES', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_NUMBER_OF_KEY_MISSING_FIELDS', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_AWARD_DATE_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_BUYER_NAME_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_PROC_METHOD_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_BUYER_LOC_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_BIDDER_ID_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_BIDDER_NAME_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MARKET_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_TITLE_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_VALUE_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_YEAR_MISSING', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_INTEGRITY_WINNER_CA_SHARE', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_ADDRESS_OF_IMPLEMENTATION_NUTS', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_ELIGIBLE_BID_LANGUAGES', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_OR_INCOMPLETE_AWARD_CRITERIA', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_OR_INCOMPLETE_CPVS', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_OR_INCOMPLETE_DURATION_INFO', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_OR_INCOMPLETE_FUNDINGS_INFO', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_SELECTION_METHOD', types.StringType(), True),\n",
    "    types.StructField('tender_indicator_TRANSPARENCY_MISSING_SUBCONTRACTED_INFO', types.StringType(), True),\n",
    "    types.StructField('buyer_row_nr', types.StringType(), True),\n",
    "    types.StructField('buyer_buyerType', types.StringType(), True),\n",
    "    types.StructField('buyer_mainActivities', types.StringType(), True),\n",
    "    types.StructField('buyer_id', types.StringType(), True),\n",
    "    types.StructField('buyer_name', types.StringType(), True),\n",
    "    types.StructField('buyer_nuts', types.StringType(), True),\n",
    "    types.StructField('buyer_city', types.StringType(), True),\n",
    "    types.StructField('buyer_country', types.StringType(), True),\n",
    "    types.StructField('buyer_postcode', types.StringType(), True),\n",
    "    types.StructField('lot_row_nr', types.StringType(), True),\n",
    "    types.StructField('lot_title', types.StringType(), True),\n",
    "    types.StructField('lot_selectionMethod', types.StringType(), True),\n",
    "    types.StructField('lot_status', types.StringType(), True),\n",
    "    types.StructField('lot_contractSignatureDate', types.StringType(), True),\n",
    "    types.StructField('lot_cancellationDate', types.StringType(), True),\n",
    "    types.StructField('lot_isAwarded', types.StringType(), True),\n",
    "    types.StructField('lot_estimatedPrice', types.FloatType(), True),\n",
    "    types.StructField('lot_estimatedPrice_currency', types.StringType(), True),\n",
    "    types.StructField('lot_estimatedPrice_minNetAmount', types.FloatType(), True),\n",
    "    types.StructField('lot_estimatedPrice_maxNetAmount', types.FloatType(), True),\n",
    "    types.StructField('lot_estimatedPrice_EUR', types.FloatType(), True),\n",
    "    types.StructField('lot_lotNumber', types.StringType(), True),\n",
    "    types.StructField('lot_bidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_validBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_smeBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_electronicBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_nonEuMemberStatesCompaniesBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_otherEuMemberStatesCompaniesBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_foreignCompaniesBidsCount', types.IntegerType(), True),\n",
    "    types.StructField('lot_description_length', types.StringType(), True),\n",
    "    types.StructField('bid_row_nr', types.StringType(), True),\n",
    "    types.StructField('bid_isWinning', types.StringType(), True),\n",
    "    types.StructField('bid_isSubcontracted', types.StringType(), True),\n",
    "    types.StructField('bid_isConsortium', types.StringType(), True),\n",
    "    types.StructField('bid_price', types.FloatType(), True),\n",
    "    types.StructField('bid_price_currency', types.StringType(), True),\n",
    "    types.StructField('bid_price_minNetAmount', FloatType(), True),\n",
    "    types.StructField('bid_price_maxNetAmount', FloatType(), True),\n",
    "    types.StructField('bid_price_EUR', types.FloatType(), True),\n",
    "    types.StructField('bidder_row_nr', types.StringType(), True),\n",
    "    types.StructField('bidder_id', types.StringType(), True),\n",
    "    types.StructField('bidder_name', types.StringType(), True),\n",
    "    types.StructField('bidder_nuts', types.StringType(), True),\n",
    "    types.StructField('bidder_city', types.StringType(), True),\n",
    "    types.StructField('bidder_country', types.StringType(), True),\n",
    "    types.StructField('bidder_postcode', types.StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 10:01:26 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\n",
      "22/04/15 10:01:26 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
      "22/04/15 10:01:26 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInRead' instead.\n",
      "22/04/15 10:01:26 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
      "22/04/15 10:01:27 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://dl-eu-pub-tender/raw_data/country_data/*.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o209.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19663/3661273731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://dl-eu-pub-tender/raw_data/country_data/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o209.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('gs://dl-eu-pub-tender/raw_data/country_data/*')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  boolean_conversion(base_str):\n",
    "    if base_str == 'yes':\n",
    "        return True\n",
    "    elif base_str == 'no':\n",
    "        return False\n",
    "    else:\n",
    "        return f'unknown'\n",
    "\n",
    "boolean_conversion_udf = F.udf(boolean_conversion, returnType=types.BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 13:33:20 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInWrite' instead.\n",
      "22/04/12 13:33:20 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
      "22/04/12 13:33:20 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.int96RebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.int96RebaseModeInRead' instead.\n",
      "22/04/12 13:33:20 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "    .withColumn('tender_date', F.to_date(df.tender_awardDecisionDate, 'yyyy-mm-dd')) \\\n",
    "    .withColumn('tender_year', F.year(df.tender_year)) \\\n",
    "    .withColumnRenamed('tender_supplyType', 'purchase_type') \\\n",
    "    .withColumnRenamed('tender_procedureType', 'procedure_type') \\\n",
    "    .withColumnRenamed('tender_finalPrice_EUR', 'final_price') \\\n",
    "    .withColumnRenamed('tender_awardCriteria_count', 'award_criteria_count') \\\n",
    "    .withColumnRenamed('buyer_buyerType', 'buyer_type') \\\n",
    "    .withColumnRenamed('buyer_mainActivities', 'buyers_activities') \\\n",
    "    .withColumn('eu_funded', boolean_conversion_udf(df.tender_isEUFunded)) \\\n",
    "    .select('tender_id', 'eu_funded', 'tender_year', 'tender_date', 'tender_country','buyer_name', 'buyer_type', 'buyers_activities', 'purchase_type', 'procedure_type','award_criteria_count', 'bidder_name', 'final_price' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tender_id: string, eu_funded: boolean, tender_year: int, tender_date: date, tender_country: string, buyer_name: string, buyer_type: string, buyers_activities: string, purchase_type: string, procedure_type: string, award_criteria_count: int, bidder_name: string, final_price: float]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('gs://dl-eu-pub-tender/raw_data/parquet', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 13:54:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         bidder_name|               value|\n",
      "+--------------------+--------------------+\n",
      "|BG TEVA GmbH, rat...|         2.092538E12|\n",
      "|   ALIUD PHARMA GmbH|    9.82911731748E11|\n",
      "|     1 A Pharma GmbH|    7.82850600616E11|\n",
      "|  Aristo Pharma GmbH|     5.4316061316E11|\n",
      "|BG Heumann Pharma...|          4.37472E11|\n",
      "|BG PUREN Pharma G...|          4.37472E11|\n",
      "|            Hexal AG|    3.92045495996E11|\n",
      "|    GALENpharma GmbH|    3.89219745296E11|\n",
      "|neuraxpharm Arzne...|    3.42314004356E11|\n",
      "|BG Zentiva Pharma...|          2.91648E11|\n",
      "|Glenmark Arzneimi...|    2.01546874736E11|\n",
      "|         Basics GmbH|1.963134267834375E11|\n",
      "|     AAA-Pharma GmbH|    1.47546933764E11|\n",
      "|         TEVA ITALIA|     1.1216921136E11|\n",
      "|AIESI HOSPITAL SE...|    1.11437296291E11|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show 15 largest public tender suppliers in EU\n",
    "df_largest_suppliers = spark.sql(\"\"\"\n",
    "SELECT bidder_name, sum(final_price) AS value\n",
    "FROM data\n",
    "WHERE bidder_name IS NOT NULL\n",
    "GROUP BY bidder_name\n",
    "ORDER BY sum(final_price) DESC\n",
    "LIMIT 15;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>(328 + 2) / 330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   buyers_activities|               value|\n",
      "+--------------------+--------------------+\n",
      "|              HEALTH|1.392437537436630...|\n",
      "|HEALTH,OTHER,GENE...|1.182682029617625...|\n",
      "|GENERAL_PUBLIC_SE...|1.678776219599417...|\n",
      "|               OTHER|1.183263491195017...|\n",
      "|        HEALTH,OTHER|8.420023862121294E11|\n",
      "|OTHER,GENERAL_PUB...|8.092877406141925E11|\n",
      "|GENERAL_PUBLIC_SE...| 6.22540863347945E11|\n",
      "|HEALTH,GENERAL_PU...|5.969822360742549E11|\n",
      "|GENERAL_PUBLIC_SE...|5.254400545768906E11|\n",
      "|OTHER,ENVIRONMENT...|2.489289820332660...|\n",
      "|GENERAL_PUBLIC_SE...|2.445128217288515...|\n",
      "|           EDUCATION|2.358245051329227...|\n",
      "|             DEFENCE|1.792321960181112...|\n",
      "|HOUSING_AND_COMMU...|1.580622228752117...|\n",
      "|HOUSING_AND_COMMU...|1.500738696063661E11|\n",
      "|HEALTH,GENERAL_PU...|1.265497186235592...|\n",
      "|PUBLIC_ORDER_AND_...|1.208172822787124...|\n",
      "|         ELECTRICITY|1.135620294280568...|\n",
      "|HEALTH,OTHER,HOUS...|1.077169679170937...|\n",
      "|GENERAL_PUBLIC_SE...|1.050897271318592...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show 25 sectors, where EU spends most\n",
    "df_largest_sectors = spark.sql(\"\"\"\n",
    "SELECT buyers_activities, sum(final_price) AS value\n",
    "FROM data\n",
    "WHERE buyers_activities IS NOT NULL\n",
    "GROUP BY buyers_activities\n",
    "ORDER BY sum(final_price) DESC\n",
    "LIMIT 25;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(329 + 1) / 330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      procedure_type|              amount|\n",
      "+--------------------+--------------------+\n",
      "|                OPEN|3.703694820816669E13|\n",
      "|          RESTRICTED|9.072365150053544E11|\n",
      "|NEGOTIATED_WITH_P...|8.352770879332925E11|\n",
      "|NEGOTIATED_WITHOU...|4.951850850196092E11|\n",
      "|          NEGOTIATED|3.547498204683302E11|\n",
      "|  COMPETITIVE_DIALOG|9.077439841323251E10|\n",
      "|                null| 5.01239254870525E10|\n",
      "| APPROACHING_BIDDERS|3.703755632541736E10|\n",
      "|INOVATION_PARTNER...|1.489559890273312...|\n",
      "|               OTHER| 9.242293528452265E9|\n",
      "|      OUTRIGHT_AWARD|3.8540221732375107E9|\n",
      "|          MINITENDER|1.1392184837360263E9|\n",
      "|   Lot 4 - DRDP Iasi|      4.5520667875E8|\n",
      "| Lotto n. 5: 0600...|         4.9800076E7|\n",
      "|      PUBLIC_CONTEST|         4.3445801E7|\n",
      "| 4. ...|          1.893635E7|\n",
      "| Construire centr...|         1.8848958E7|\n",
      "|  ...|          1.531716E7|\n",
      "|  ...|         1.4640822E7|\n",
      "|  4-  ...|              1.05E7|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show purchases by procedure type\n",
    "df_procedures = spark.sql(\"\"\"\n",
    "SELECT procedure_type, sum(final_price) AS amount\n",
    "FROM data\n",
    "GROUP BY procedure_type\n",
    "ORDER BY sum(final_price) DESC\n",
    "LIMIT 12;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====================================================>(328 + 2) / 330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------------+\n",
      "|tender_country|tender_year|               value|\n",
      "+--------------+-----------+--------------------+\n",
      "|            IT|       2016|1.228789393257906...|\n",
      "|            DE|       2016|9.886196574122207E12|\n",
      "|            FR|       2017|8.020394195745898E11|\n",
      "|            FR|       2019|7.826629803352505E11|\n",
      "|            FR|       2018|6.614133934468223E11|\n",
      "|            FR|       2016|5.684921082847106E11|\n",
      "|            LT|       2015|4.549899950147812...|\n",
      "|            PL|       2019|4.316972988045312...|\n",
      "|            IT|       2018| 3.95641994436104E11|\n",
      "|            PL|       2018|3.528840293357969E11|\n",
      "|            ES|       2018|3.105747164751419E11|\n",
      "|            FR|       2020|3.005226664332304...|\n",
      "|            IT|       2019|2.953583413658703...|\n",
      "|            IT|       2015|2.953137557538583...|\n",
      "|            PL|       2017|2.803455343799687...|\n",
      "|            RO|       2017|2.754602780603906E11|\n",
      "|            IT|       2017|2.442633661429955...|\n",
      "|            ES|       2019|2.381609711489872...|\n",
      "|            RO|       2014|2.255745434481718...|\n",
      "|            PL|       2014|2.205679956475156...|\n",
      "+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show tender values by country over years\n",
    "df_values_by_country = spark.sql(\"\"\"\n",
    "SELECT tender_country, tender_year, sum(final_price) AS value\n",
    "FROM data\n",
    "GROUP BY tender_country, tender_year\n",
    "ORDER BY sum(final_price) DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=====================================================>(328 + 2) / 330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|tender_date|tender_year|          buyer_name|   buyers_activities|bidder_name|  final_price|\n",
      "+-----------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|       null|       2016|Azienda Ospedalie...|HEALTH,OTHER,GENE...|     Abbvie|1.09164569E11|\n",
      "+-----------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show largest public tender ever\n",
    "df_largest_tender = spark.sql(\"\"\"\n",
    "SELECT tender_date, tender_year, buyer_name, buyers_activities, bidder_name, final_price\n",
    "FROM data\n",
    "ORDER BY final_price DESC\n",
    "LIMIT 1;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how largest buyer revenues develop over years\n",
    "df_largest_bidders_revenues = spark.sql(\"\"\"\n",
    "SELECT bidder_name, tender_year, sum(final_price) AS revenue\n",
    "FROM data\n",
    "GROUP BY bidder_name, tender_year\n",
    "ORDER BY sum(final_price) DESC\n",
    "LIMIT 15;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                       (4 + 4) / 330]\r"
     ]
    }
   ],
   "source": [
    "df_largest_bidders_revenues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'coalesce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6605/471763532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_largest_sectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'largest_sectors.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'coalesce'"
     ]
    }
   ],
   "source": [
    "df_largest_sectors.coalesce(1) \\\n",
    "    .write.parquet(output/+'largest_sectors.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures.coalesce(1) \\\n",
    "    .write.parquet(output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values_by_country.coalesce(1) \\\n",
    "    .write.parquet(output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_largest_tender.coalesce(1) \\\n",
    "    .write.parquet(output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_largest_bidders_revenues.coalesce(1) \\\n",
    "    .write.parquet(output, mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17ee5042bd2f374ce893bd79ee2f892fd6d3a6c485cf6ba457a8346252691a2a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
